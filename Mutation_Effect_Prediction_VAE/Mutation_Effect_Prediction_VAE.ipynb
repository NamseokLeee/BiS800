{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j7etMUP_VJv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "\n",
        "from scipy.stats import spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loading one-hot encoded beta-lactamase and IF1 datasets and spliting into training and test sets.\n",
        "\"\"\"\n",
        "\n",
        "batch_size=64\n",
        "split = 0.95\n",
        "\n",
        "\n",
        "BL_datas= np.load('BL_train_data.npy')\n",
        "np.random.shuffle(BL_datas)\n",
        "train_data = BL_datas[ : int(BL_datas.shape[0]*split)]\n",
        "test_data = BL_datas[int(BL_datas.shape[0]*split): ]\n",
        "BL_train_data = tf.data.Dataset.from_tensor_slices(train_data).shuffle(len(train_data)).batch(batch_size)\n",
        "BL_test_data = tf.data.Dataset.from_tensor_slices(test_data).shuffle(len(test_data)).batch(batch_size)\n",
        "\n",
        "\n",
        "BL_mutation_data = np.load('BL_test_data.npy')\n",
        "BL_mutation_values = np.load('BL_target_values.npy')"
      ],
      "metadata": {
        "id": "z_audJci_X6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "split = 0.95\n",
        "\n",
        "\n",
        "IF1_datas= np.load('IF1_train_data.npy')\n",
        "np.random.shuffle(IF1_datas)\n",
        "train_data = IF1_datas[ : int(IF1_datas.shape[0]*split)]\n",
        "test_data = IF1_datas[int(IF1_datas.shape[0]*split): ]\n",
        "IF1_train_data = tf.data.Dataset.from_tensor_slices(train_data).shuffle(len(train_data)).batch(batch_size)\n",
        "IF1_test_data = tf.data.Dataset.from_tensor_slices(test_data).shuffle(len(test_data)).batch(batch_size)\n",
        "\n",
        "\n",
        "IF1_mutation_data = np.load('IF1_test_data.npy')\n",
        "IF1_mutation_values = np.load('IF1_target_values.npy')"
      ],
      "metadata": {
        "id": "zcJIW8CP_X4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function that makes sure only 20 canonical residues are used in peptide sequences.\n",
        "\"\"\"\n",
        "def cleanup(ListofSeqs):\n",
        "  cleaned_up=[]\n",
        "  \n",
        "  dash=np.zeros(20)\n",
        "  \n",
        "  Z=np.zeros(20)\n",
        "  Z[8]=0.5\n",
        "  Z[10]=0.5\n",
        "  \n",
        "  B=np.zeros(20)\n",
        "  B[7]=0.5\n",
        "  B[9]=0.5\n",
        "\n",
        "  X=np.ones(20)\n",
        "  X=X/20\n",
        "\n",
        "  for seq in ListofSeqs:\n",
        "    cleanedSeq=[]\n",
        "    for alphabet in seq:\n",
        "      if alphabet[0]==1:\n",
        "        cleanedSeq.append(dash)\n",
        "      elif alphabet[1]==1:\n",
        "        cleanedSeq.append(Z)\n",
        "      elif alphabet[2]==1:\n",
        "        cleanedSeq.append(B)\n",
        "      elif alphabet[23]==1:\n",
        "        cleanedSeq.append(X)\n",
        "      else:\n",
        "        cleanedSeq.append(alphabet[3:23])\n",
        "    cleaned_up.append(cleanedSeq)\n",
        "\n",
        "  return np.array(cleaned_up)"
      ],
      "metadata": {
        "id": "xzfVlyXR_X2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Alternative version where sequences containing Z, B, X are simply discarded.\n",
        "\"\"\"\n",
        "def cleanupByRemoval(ListofSeqs):\n",
        "  cleaned_up=[]\n",
        "\n",
        "  for seq in ListofSeqs:\n",
        "    cleanedSeq=[]\n",
        "    check=True\n",
        "    for alphabet in seq:\n",
        "      if alphabet[0]==1:\n",
        "        cleanedSeq.append(np.zeros(20))\n",
        "      elif alphabet[1]==1:\n",
        "        check=False\n",
        "        break\n",
        "      elif alphabet[2]==1:\n",
        "        check=False\n",
        "        break\n",
        "      elif alphabet[23]==1:\n",
        "        check=False\n",
        "        break\n",
        "      else:\n",
        "        cleanedSeq.append(alphabet[3:23])\n",
        "    \n",
        "    if check:\n",
        "      cleaned_up.append(cleanedSeq)\n",
        "\n",
        "  return np.array(cleaned_up)\n"
      ],
      "metadata": {
        "id": "VFjDPvv7_X0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(BL_datas.shape)\n",
        "BL_datas=cleanupByRemoval(BL_datas)\n",
        "print(BL_datas.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRbhImtr_XzD",
        "outputId": "996cfafb-b802-4531-97eb-8a8c5e306f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8403, 253, 24)\n",
            "(8355, 253, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split=0.95\n",
        "np.random.shuffle(BL_datas)\n",
        "train_data = BL_datas[ : int(BL_datas.shape[0]*split)]\n",
        "test_data = BL_datas[int(BL_datas.shape[0]*split): ]\n",
        "BL_train_data = tf.data.Dataset.from_tensor_slices(train_data).shuffle(len(train_data)).batch(batch_size)\n",
        "BL_test_data = tf.data.Dataset.from_tensor_slices(test_data).shuffle(len(test_data)).batch(batch_size)"
      ],
      "metadata": {
        "id": "4pIz6CpZ_XxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(BL_mutation_data.shape)\n",
        "BL_mutation_data = cleanup(BL_mutation_data)\n",
        "print(BL_mutation_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAY3VWZo_Xvp",
        "outputId": "cfd0bb99-8e5c-4ec0-fc29-5206ff51a479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4610, 253, 24)\n",
            "(4610, 253, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import concat\n",
        "\"\"\"\n",
        "VAE model consists of inference net and generative net, and latent variables of \n",
        "given dimension connects the two parts.\n",
        "This model is a doubly variational model where the generative net is also a variational network\n",
        "in addition to the variational latent dimension sampling.\n",
        "The architecture of this model is from this paper:\n",
        "Riesselman, A.J., Ingraham, J.B. & Marks, D.S. Deep generative models of genetic \n",
        "variation capture the effects of mutations. Nat Methods 15, 816â€“822 (2018).\n",
        "\"\"\"\n",
        "\n",
        "class VAEsvi(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(VAEsvi, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.inference_net = tf.keras.Sequential(\n",
        "      [\n",
        "          tf.keras.layers.InputLayer(input_shape=(253, 20)),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(1500, activation='relu'),\n",
        "          tf.keras.layers.Dense(1500, activation='relu'),\n",
        "          tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    create_weight = lambda dim_input, dim_output: tf.random.normal(\n",
        "        [dim_input, dim_output],\n",
        "        mean = 0.0,\n",
        "        stddev = tf.math.sqrt(2.0 / (dim_input + dim_output))\n",
        "        )\n",
        "\n",
        "    create_bias = lambda dim_output:  0.1 * np.ones(dim_output)\n",
        "\n",
        "    self.logsig_init = -5\n",
        "\n",
        "    create_weight_logsig = lambda dim_input, dim_output: self.logsig_init * np.ones((dim_input, dim_output))\n",
        "    \n",
        "    create_bias_logsig = lambda dim_output: self.logsig_init * np.ones(dim_output)\n",
        "\n",
        "    self.W1 = tf.Variable(create_weight(latent_dim, 100), trainable=True)\n",
        "    self.W1_logsig = tf.Variable(create_weight_logsig(latent_dim, 100), dtype='float32', trainable=True)\n",
        "    self.B1 = tf.Variable(create_bias(100), dtype='float32', trainable=True)\n",
        "    self.B1_logsig = tf.Variable(create_bias_logsig(100), dtype='float32', trainable=True)\n",
        "\n",
        "    self.W2 = tf.Variable(create_weight(100, 2000), trainable=True)\n",
        "    self.W2_logsig = tf.Variable(create_weight_logsig(100, 2000), dtype='float32', trainable=True)\n",
        "    self.B2 = tf.Variable(create_bias(2000), dtype='float32', trainable=True)\n",
        "    self.B2_logsig = tf.Variable(create_bias_logsig(2000), dtype='float32', trainable=True)\n",
        "\n",
        "    self.W3 = tf.Variable(create_weight(2000, 253*20), trainable=True)\n",
        "    self.W3_logsig = tf.Variable(create_weight_logsig(2000, 253*20), dtype='float32', trainable=True)\n",
        "    self.B3 = tf.Variable(create_bias(253*20), dtype='float32', trainable=True)\n",
        "    self.B3_logsig = tf.Variable(create_bias_logsig(253*20), dtype='float32', trainable=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def reparameterize2(self, mean, logsig):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logsig) + mean\n",
        "\n",
        "  def decode(self, z):\n",
        "    w1 = self.reparameterize2(self.W1,self.W1_logsig)\n",
        "    w2 = self.reparameterize2(self.W2,self.W2_logsig)\n",
        "    w3 = self.reparameterize2(self.W3,self.W3_logsig)\n",
        "    b1 = self.reparameterize2(self.B1,self.B1_logsig)\n",
        "    b2 = self.reparameterize2(self.B2,self.B2_logsig)\n",
        "    b3 = self.reparameterize2(self.B3,self.B3_logsig)\n",
        "    \n",
        "    dense1 = tf.nn.relu(tf.matmul(z, w1) + b1)\n",
        "    dense2 = tf.nn.sigmoid(tf.matmul(dense1, w2) + b2)\n",
        "    logits = tf.matmul(dense2, w3) + b3\n",
        "    return tf.reshape(logits, shape=[logits.shape[0],253,20])\n",
        "  \n",
        "  def gen_kld_params(self):\n",
        "    KLD_params = 0.0\n",
        "    for i, j in [(self.W1, self.W1_logsig),\n",
        "                 (self.W2, self.W2_logsig),\n",
        "                 (self.W3, self.W3_logsig),\n",
        "                 (self.B1, self.B1_logsig),\n",
        "                 (self.B2, self.B2_logsig),\n",
        "                 (self.B3, self.B3_logsig)]:\n",
        "        mu = tf.reshape(i, [-1])\n",
        "        log_sigma = tf.reshape(j, [-1])\n",
        "        KLD_params += tf.reduce_sum(-self.KLD_diag_gaussians(mu, log_sigma, 0.0, 1.0), -1)\n",
        "\n",
        "    return KLD_params\n",
        "\n",
        "  def KLD_diag_gaussians(self, mu, log_sigma, prior_mu, prior_log_sigma):\n",
        "    return prior_log_sigma - log_sigma + 0.5 * (tf.math.exp(2. * log_sigma) \\\n",
        "            + tf.math.square(mu - prior_mu)) * tf.math.exp(-2. * prior_log_sigma) - 0.5\n"
      ],
      "metadata": {
        "id": "8t2tEaSt_Xtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim=30\n",
        "model = VAEsvi(latent_dim)\n",
        "model.inference_net.summary()\n",
        "len(model.trainable_variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvb_I_FJ_Xrp",
        "outputId": "272cd41e-4459-4ff1-aae9-41c87ffbcc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 5060)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1500)              7591500   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1500)              2251500   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 60)                90060     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,933,060\n",
            "Trainable params: 9,933,060\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "log_normal_pdf function is used to easy calculation of KL divergence, and compute_loss\n",
        "function calculates ELBO.\n",
        "Given sequence (x) is used to estimate q(z|x) distribution and reprametrization trick \n",
        "is utilized to randomly sample a z value.\n",
        "Then, x is chosen from p(x|z) distribution and compared with input protein sequence. \n",
        "Softmax cross entropy is used to calculate errors.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "\n",
        "  cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=1)\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "  return -(tf.reduce_sum(logpx_z + logpz - logqz_x) + model.gen_kld_params())/x.shape[0]\n",
        "\n",
        "@tf.function\n",
        "def compute_apply_gradients(model, x, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "metadata": {
        "id": "ibJRflap_Xm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training the model.\n",
        "\"\"\"\n",
        "\n",
        "epochs = 100000\n",
        "tot_time=0\n",
        "prevelbo=-1000000000\n",
        "count=0\n",
        "seedcount=0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  np.random.seed(seedcount)\n",
        "  start_time = time.time()\n",
        "  for train_x in BL_train_data:\n",
        "    compute_apply_gradients(model, train_x, optimizer)\n",
        "\n",
        "  end_time = time.time()\n",
        "  tot_time+=(end_time-start_time)\n",
        "\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    loss = tf.keras.metrics.Mean()\n",
        "    for test_x in BL_test_data:\n",
        "      loss(compute_loss(model, test_x))\n",
        "    elbo = -loss.result()\n",
        "    if elbo <= prevelbo:\n",
        "      count+=1\n",
        "    else:\n",
        "      prevelbo=elbo\n",
        "      count=0\n",
        "    display.clear_output(wait=False)\n",
        "    print('Epoch: {}, Test set ELBO: {}, '\n",
        "          'time elapse for current epoch {} sec, Total elapsed time {} sec'.format(epoch,\n",
        "                                                    elbo,\n",
        "                                                    end_time - start_time, tot_time))\n",
        "    if epoch % 100 ==0:\n",
        "      model.save_weights('./checkpointsforever4/my_checkpointforever4')\n",
        "\n",
        "\n",
        "  seedcount+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "cLJX1bG3_Xk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WTseq=np.array(list(BL_mutation_data[0]))\n",
        "WTseq[0]=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]"
      ],
      "metadata": {
        "id": "AWy3TKtqL8Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_p(model, x, iteration):\n",
        "  mean, logvar = model.encode(x)\n",
        "  KLD_latent = 0.5 * tf.reduce_sum(1.0 + logvar - mean**2.0 - tf.exp(logvar), axis=1)\n",
        "\n",
        "  tileFactor = tf.constant([iteration,1])\n",
        "  k1 = tf.tile(mean, tileFactor)\n",
        "  k2 = tf.tile(logvar, tileFactor)\n",
        "\n",
        "\n",
        "\n",
        "  z = model.reparameterize(k1, k2)\n",
        "  x_logit = model.decode(z)\n",
        "  cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=x_logit, labels=tf.tile(x, tf.constant([iteration,1,1])))\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=1)\n",
        "  temp = tf.reshape(logpx_z, [iteration, x.shape[0]])\n",
        "  logpx_z_Sum = tf.reduce_sum(temp, axis=0)\n",
        "  \n",
        "  return KLD_latent + logpx_z_Sum/iteration"
      ],
      "metadata": {
        "id": "SplS7Gzf_Xg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[]\n",
        "tmp=0\n",
        "Iteration = 100\n",
        "\n",
        "logpWT = -compute_p(model, tf.constant([WTseq]), Iteration)\n",
        "logpM= -compute_p(model, BL_mutation_data, Iteration)\n",
        "a= (logpM-logpWT).numpy()\n",
        "\n",
        "rho, p = spearmanr(a, BL_mutation_values)\n",
        "print(\"Spearman Correlation: \", rho, \"\\nP Val:\", p)"
      ],
      "metadata": {
        "id": "-hsYyJwQL5Vq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}